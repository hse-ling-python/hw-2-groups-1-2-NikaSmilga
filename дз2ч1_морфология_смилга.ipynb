{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 2. Часть 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Я проверяла код пепом, ошибок не найдено, но с пепом не работает вывод и замер времени, поэтому в итоговой версии я его убрала (это посоветовали в беседе в Телеграме)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymystem3 in ./anaconda3/lib/python3.7/site-packages (0.2.0)\r\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.7/site-packages (from pymystem3) (2.22.0)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./anaconda3/lib/python3.7/site-packages (from requests->pymystem3) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./anaconda3/lib/python3.7/site-packages (from requests->pymystem3) (1.24.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.7/site-packages (from requests->pymystem3) (2019.6.16)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./anaconda3/lib/python3.7/site-packages (from requests->pymystem3) (2.8)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in ./anaconda3/lib/python3.7/site-packages (0.8)\n",
      "Requirement already satisfied: dawg-python>=0.7 in ./anaconda3/lib/python3.7/site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in ./anaconda3/lib/python3.7/site-packages (from pymorphy2) (2.4.393442.3710985)\n",
      "Requirement already satisfied: docopt>=0.6 in ./anaconda3/lib/python3.7/site-packages (from pymorphy2) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Задание 2._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем модули, которые понадобятся нам для работы. Открываем файл с текстом книги *hateandloathing.txt* и записываем текст в переменную *text*. Анализируем текст с помощью майстема и запишем его в переменную *text_mystem*. Записываем его в файл *hate_and_loathing_mystem.json* с помощью *json.dump*. Время засекаем в самом начале с помощью *%%time* -- так я буду делать и для каждого следующего задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 110 ms, total: 1.29 s\n",
      "Wall time: 5.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import json\n",
    "from pymystem3 import Mystem\n",
    "with open('hateandloathing.txt', 'r', encoding = 'utf-8') as a:\n",
    "    text = a.read()\n",
    "m = Mystem()\n",
    "text_mystem = m.analyze(text)\n",
    "with open('hate_and_loathing_mystem.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(text_mystem, f, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Задание 3._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем модули, нужные нам для работы с пайморфи. Создаем пустые список и словарь. Токенизируем текст книги, вместе с этим избавляясь от знаков препинания и заменяя заглавные буквы строчными. Наши токены будут содержаться в списке *text_py*. Каждый токен в этом списке парсим и вынимаем из него первый вариант разбора. Из первого варианта разбора достаем само слово, его лемму и его часть речи. Сразу же записываем их в словарь *dicti* -- само слово как ключ, остальное -- как его значения. Далее записываем весь получившийся словарь в файл *hate_and_loathing_py.json* через *json.dump*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.9 s, sys: 510 ms, total: 12.5 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text_py1 = []\n",
    "morph = MorphAnalyzer()\n",
    "text_py = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "text_len = len(text_py)\n",
    "dicti = {}\n",
    "for word in text_py:\n",
    "    new_word = morph.parse(word)\n",
    "    new_word1 = new_word[0]\n",
    "    dicti[new_word1.word] = new_word1.normal_form, new_word1.tag.POS\n",
    "with open('hate_and_loathing_py.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dicti, f, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Задание 4.1._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем список *pos* со всеми тегами частей речи, которые вообще есть в документации НЛТК. Их 17. Запускаем цикл: для каждого элемента списка создаем нулевой счетчик и запускаем циклы для слов из разобранного на токены текста. Для каждого слова делаем морф. разбор и вынимаем из него первый вариант разбора. Если проверяемая в данной итерации цикл часть речи есть в этом разборе, прибавляем к счетчику 1. Выходим из внутреннего цикла и для каждой части речи печатаем ее процент (значение ее счетчика разделить на кол-во слов в тексте, посчитанное ранее, и умножить на сто.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN = 25.054787592717464 %\n",
      "ADJF = 11.711901550910317 %\n",
      "ADJS = 1.0009271746459878 %\n",
      "COMP = 0.26340188806473364 %\n",
      "VERB = 12.97623061362104 %\n",
      "INFN = 3.1334288604180713 %\n",
      "PRTF = 1.0873229939312203 %\n",
      "PRTS = 0.2718307484828051 %\n",
      "GRND = 0.981962238705327 %\n",
      "NUMR = 0.8344571813890762 %\n",
      "ADVB = 6.115138233310857 %\n",
      "NPRO = 10.4243931220499 %\n",
      "PRED = 0.4846594740391099 %\n",
      "PREP = 11.288351314902226 %\n",
      "CONJ = 8.763907619689817 %\n",
      "PRCL = 4.840273095077546 %\n",
      "INTJ = 0.2676163182737694 %\n",
      "CPU times: user 3min 15s, sys: 635 ms, total: 3min 15s\n",
      "Wall time: 3min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pos = ['NOUN', 'ADJF', 'ADJS', 'COMP', 'VERB', 'INFN', 'PRTF',\n",
    "        'PRTS', 'GRND', 'NUMR', 'ADVB', 'NPRO', 'PRED', 'PREP', \n",
    "        'CONJ', 'PRCL', 'INTJ']\n",
    "for token in pos:\n",
    "    counter = 0\n",
    "    for word in text_py:\n",
    "        new_word = morph.parse(word)\n",
    "        new_word1 = new_word[0]\n",
    "        if token in new_word1.tag:\n",
    "            counter += 1\n",
    "    print(token, '=', (counter/text_len)*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Задание 4.1._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем два пустых списка: для глаголов, *all_verbs*, и для наречий, *all_adverbs*. Для каждого слова из разобранного на токены текста делаем морф. разбор, и если это глагол, добавляем его в список глаголов. Аналогично для наречий. Затем применяем метод *Counter* к получившемуся списку глаголов. Используем *most_common*, чтобы вынуть 20 наиболее часто встречающихся глаголов. Распечатываем топ глаголов, получив их места в топе с помощью нового счетчика *counter_v* (такое я уже делала и объянсяла в предыдущем дз). Аналогично для наречий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТОП-20 ГЛАГОЛОВ:\n",
      "\n",
      "1. быть: 494\n",
      "2. сказать: 221\n",
      "3. мочь: 203\n",
      "4. хотеть: 79\n",
      "5. знать: 70\n",
      "6. взять: 54\n",
      "7. спросить: 53\n",
      "8. сделать: 49\n",
      "9. видеть: 46\n",
      "10. иметь: 45\n",
      "11. думать: 45\n",
      "12. начать: 43\n",
      "13. заметить: 41\n",
      "14. оказаться: 40\n",
      "15. кивнуть: 35\n",
      "16. чувствовать: 34\n",
      "17. говорить: 33\n",
      "18. смочь: 33\n",
      "19. давать: 31\n",
      "20. делать: 31\n",
      "\n",
      "ТОП-20 НАРЕЧИЙ:\n",
      "\n",
      "1. уже: 106\n",
      "2. прямо: 98\n",
      "3. здесь: 97\n",
      "4. где: 83\n",
      "5. там: 74\n",
      "6. сейчас: 68\n",
      "7. никогда: 65\n",
      "8. пока: 61\n",
      "9. затем: 51\n",
      "10. снова: 46\n",
      "11. слишком: 46\n",
      "12. потом: 44\n",
      "13. всего: 39\n",
      "14. очень: 38\n",
      "15. потому: 38\n",
      "16. немного: 36\n",
      "17. назад: 35\n",
      "18. несколько: 30\n",
      "19. теперь: 29\n",
      "20. быстро: 29\n",
      "CPU times: user 10.3 s, sys: 78.6 ms, total: 10.3 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import collections\n",
    "all_verbs = []\n",
    "all_adverbs = []\n",
    "for word in text_py:\n",
    "    new_word = morph.parse(word)\n",
    "    new_word1 = new_word[0]\n",
    "    if 'VERB' in new_word1.tag:\n",
    "        all_verbs.append(new_word1.normal_form)\n",
    "    if 'ADVB' in new_word1.tag:\n",
    "        all_adverbs.append(new_word1.normal_form)\n",
    "dicti_verbs = collections.Counter(all_verbs)\n",
    "top_verbs = dicti_verbs.most_common(20)\n",
    "print(\"ТОП-20 ГЛАГОЛОВ:\\n\")\n",
    "counter_v = 0\n",
    "for verb in top_verbs:\n",
    "    counter_v += 1\n",
    "    print(str(counter_v) + '.', verb[0] + ':', verb[1])\n",
    "dicti_adverbs = collections.Counter(all_adverbs)\n",
    "top_adverbs = dicti_adverbs.most_common(20)\n",
    "print(\"\\nТОП-20 НАРЕЧИЙ:\\n\")\n",
    "counter_a = 0\n",
    "for adverb in top_adverbs:\n",
    "    counter_a += 1\n",
    "    print(str(counter_a) + '.', adverb[0] + ':', adverb[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Задание 5._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем пустой список *spisok_lemm*. Токенизируем текст книги, вместе с этим избавляясь от знаков препинания и заменяя заглавные буквы строчными. Наши токены будут содержаться в списке *lemmas*. Для каждого слова в этом списке проводим морф. разбор, вынимаем из получившегося первый разбор, а из него -- лемму и записываем каждую лемму в список *spisok_lemm*. Далее к этому списку применяем *nltk.bigrams()*, делаем из результатов список *dicti_bi*, а из списка частотный словарь. Используем *most_common*, чтобы вынуть 25 наиболее часто встречающихся биграмм и получить словарь *top_bi*. Распечатываем топ биграмм, получив их места в топе с помощью нового счетчика *counter_bi*. Аналогично для триграмм, только с применением *nltk.trigrams()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ТОП-25 БИГРАММ:\n",
      "\n",
      "1. ('мой', 'адвокат'): 142\n",
      "2. ('сказать', 'я'): 84\n",
      "3. ('что', 'я'): 77\n",
      "4. ('я', 'не'): 65\n",
      "5. ('в', 'это'): 64\n",
      "6. ('не', 'мочь'): 54\n",
      "7. ('сказать', 'он'): 54\n",
      "8. ('что', 'он'): 53\n",
      "9. ('не', 'быть'): 49\n",
      "10. ('и', 'я'): 43\n",
      "11. ('я', 'быть'): 42\n",
      "12. ('это', 'быть'): 42\n",
      "13. ('что', 'мы'): 42\n",
      "14. ('у', 'я'): 41\n",
      "15. ('никогда', 'не'): 41\n",
      "16. ('один', 'из'): 40\n",
      "17. ('весь', 'что'): 37\n",
      "18. ('в', 'свой'): 37\n",
      "19. ('потому', 'что'): 36\n",
      "20. ('в', 'тот'): 36\n",
      "21. ('и', 'не'): 35\n",
      "22. ('даже', 'не'): 35\n",
      "23. ('что', 'ты'): 35\n",
      "24. ('так', 'что'): 34\n",
      "25. ('он', 'я'): 34\n",
      "\n",
      "ТОП-25 ТРИГРАММ:\n",
      "\n",
      "1. ('на', 'самый', 'дело'): 19\n",
      "2. ('к', 'тот', 'время'): 18\n",
      "3. ('иметь', 'в', 'вид'): 16\n",
      "4. ('по', 'крайний', 'мера'): 15\n",
      "5. ('с', 'откидной', 'верхом'): 13\n",
      "6. ('в', 'это', 'город'): 12\n",
      "7. ('я', 'знать', 'что'): 12\n",
      "8. ('я', 'не', 'мочь'): 11\n",
      "9. ('так', 'что', 'я'): 10\n",
      "10. ('тот', 'время', 'как'): 10\n",
      "11. ('на', 'этот', 'раз'): 10\n",
      "12. ('конференция', 'окружный', 'прокурор'): 10\n",
      "13. ('великий', 'красный', 'акула'): 9\n",
      "14. ('до', 'тот', 'как'): 9\n",
      "15. ('на', 'задний', 'сидение'): 9\n",
      "16. ('весь', 'что', 'я'): 9\n",
      "17. ('почему', 'бы', 'и'): 9\n",
      "18. ('в', 'конец', 'конец'): 9\n",
      "19. ('я', 'не', 'знать'): 9\n",
      "20. ('в', 'тот', 'что'): 9\n",
      "21. ('сказать', 'мой', 'адвокат'): 8\n",
      "22. ('понятие', 'не', 'иметь'): 8\n",
      "23. ('как', 'твой', 'адвокат'): 8\n",
      "24. ('твой', 'адвокат', 'я'): 8\n",
      "25. ('адвокат', 'я', 'советовать'): 8\n",
      "CPU times: user 10.4 s, sys: 68.5 ms, total: 10.5 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk import bigrams\n",
    "from string import punctuation \n",
    "spisok_lemm = []\n",
    "lemmas = text.lower()\n",
    "lemmas = [w.lower() for w in word_tokenize(lemmas) if w.isalpha()]\n",
    "for lemma in lemmas:\n",
    "    lemma1 = morph.parse(lemma)\n",
    "    lemma1 = lemma1[0]\n",
    "    spisok_lemm.append(lemma1.normal_form)\n",
    "bigrams = nltk.bigrams(spisok_lemm)\n",
    "dicti_bi = list(bigrams)\n",
    "counter_bi = collections.Counter(dicti_bi)\n",
    "top_bi = counter_bi.most_common(25)\n",
    "counter_bi = 0\n",
    "print(\"ТОП-25 БИГРАММ:\\n\")\n",
    "for bi in top_bi:\n",
    "    counter_bi += 1\n",
    "    print(str(counter_bi) + '.', str(bi[0]) + ':', str(bi[1]))\n",
    "trigrams = nltk.trigrams(spisok_lemm)\n",
    "dicti_tri = list(trigrams)\n",
    "counter_tri = collections.Counter(dicti_tri)\n",
    "top_tri = counter_tri.most_common(25)\n",
    "counter_tri = 0\n",
    "print(\"\\nТОП-25 ТРИГРАММ:\\n\")\n",
    "for tri in top_tri:\n",
    "    counter_tri += 1\n",
    "    print(str(counter_tri) + '.', str(tri[0]) + ':', str(tri[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
